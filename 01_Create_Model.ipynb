{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "In this notebook, we create the training script whose hyperparameters will be tuned. The notebook cells are each appended in turn in the training script.\n",
    "\n",
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile TrainTestClassifier.py\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append TrainTestClassifier.py\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at provided\n",
    "    key(s).\n",
    "\n",
    "    The data are expected to be stored in a 2D data structure, where\n",
    "    the first index is over features and the second is over samples,\n",
    "    i.e.\n",
    "\n",
    "    >> len(data[keys]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn\n",
    "    feature matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[keys]).  Examples include: a dict of lists, 2D numpy array,\n",
    "    Pandas DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample\n",
    "    (e.g. a list of dicts).  If your data are structured this way,\n",
    "    consider a transformer along the lines of\n",
    "    `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    keys : hashable or list of hashable, required\n",
    "        The key(s) corresponding to the desired value(s) in a mappable.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, *args, **kwargs):\n",
    "        if type(self.keys) is list:\n",
    "            assert all([key in x for key in self.keys]), 'Not all keys in data'\n",
    "        else:\n",
    "            assert self.keys in x, 'key not in data'\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict, *args, **kwargs):\n",
    "        return data_dict[self.keys]\n",
    "\n",
    "    \n",
    "def score_rank(scores):\n",
    "    return pd.Series(scores).rank(ascending=False)\n",
    "\n",
    "\n",
    "def label_index(label, label_order):\n",
    "    loc = np.where(label == label_order)[0]\n",
    "    if loc.shape[0] == 0:\n",
    "        return None\n",
    "    return loc[0]\n",
    "\n",
    "\n",
    "def label_rank(label, scores, label_order):\n",
    "    loc = label_index(label, label_order)\n",
    "    if loc is None:\n",
    "        return len(scores) + 1\n",
    "    return score_rank(scores)[loc]\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input parameters\n",
    "One of the most important parameters is `estimators`, the number of estimators that allows you to trade-off accuracy, modeling time, and model size. The table below should give you an idea of the relationships between the number of estimators and the metrics.\n",
    "\n",
    "| Estimators | Run time (s) | Size (MB) | Accuracy@1 | Accuracy@2 | Accuracy@3 |\n",
    "|------------|--------------|-----------|------------|------------|------------|\n",
    "|        100 |           40 |  2 | 25.02% | 38.72% | 47.83% |\n",
    "|       1000 |          177 |  4 | 46.79% | 60.80% | 69.11% |\n",
    "|       2000 |          359 |  7 | 51.38% | 65.93% | 73.09% |\n",
    "|       4000 |          628 | 12 | 53.39% | 67.40% | 74.74% |\n",
    "|       8000 |          904 | 22 | 54,62% | 67.77% | 75.35% |\n",
    "\n",
    "Other parameters that may be useful to tune include the following:\n",
    "* `ngrams`: the maximum n-gram size for features, an integer ranging from 1,\n",
    "* `min_child_samples`: the minimum number of samples in a leaf, an integer ranging from 1,\n",
    "* `match`: the maximum number of training examples per duplicate question, an integer ranging from 2, and\n",
    "* `unweighted`: whether to use sample weights to compensate for unbalanced data, a boolean.\n",
    "\n",
    "The performance of the estimator is estimated on held-aside test data, and the statistic reported is how far down the list of sorted results is the correct result found. The `rank` parameter controls the maximum distance down the list for which the statistic is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append TrainTestClassifier.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Fit and evaluate a model'\n",
    "                                     ' based on train-test datasets.')\n",
    "    parser.add_argument('--data', help='the training dataset name',\n",
    "                        default='balanced_pairs_train.tsv')\n",
    "    parser.add_argument('--test', help='the test dataset name',\n",
    "                        default='balanced_pairs_test.tsv')\n",
    "    parser.add_argument('--estimators',\n",
    "                        help='the number of learner estimators',\n",
    "                        type=int, default=100)\n",
    "    parser.add_argument('--min_child_samples',\n",
    "                        help='the minimum number of samples in a child(leaf)',\n",
    "                        type=int, default=20)\n",
    "    parser.add_argument('--ngrams',\n",
    "                        help='the maximum size of word ngrams',\n",
    "                        type=int, default=1)\n",
    "    parser.add_argument('--match',\n",
    "                        help='the maximum number of duplicate matches',\n",
    "                        type=int, default=20)\n",
    "    parser.add_argument('--unweighted',\n",
    "                        help='do not use instance weights',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--rank',\n",
    "                        help='the maximum rank of correct answers',\n",
    "                        type=int, default=3)\n",
    "    parser.add_argument('--inputs', help='the inputs directory',\n",
    "                        default='.')\n",
    "    parser.add_argument('--outputs', help='the outputs directory',\n",
    "                        default='.')\n",
    "    parser.add_argument('--save', help='save the model',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--model', help='the model file', default='model.pkl')\n",
    "    parser.add_argument('--instances', help='the instances file',\n",
    "                        default='inst.txt')\n",
    "    parser.add_argument('--labels', help='the labels file',\n",
    "                        default='labels.txt')\n",
    "    parser.add_argument('--verbose',\n",
    "                        help='the verbosity of the estimator',\n",
    "                        type=int, default=-1)\n",
    "    args = parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append TrainTestClassifier.py\n",
    "\n",
    "    print('Prepare the training data.')\n",
    "    \n",
    "    # Paths to the input data.\n",
    "    inputs_path = args.inputs\n",
    "    data_path = os.path.join(inputs_path, args.data)\n",
    "    test_path = os.path.join(inputs_path, args.test)\n",
    "\n",
    "    # Paths for the output data.\n",
    "    outputs_path = args.outputs\n",
    "    model_path = os.path.join(outputs_path, args.model)\n",
    "    instances_path = os.path.join(outputs_path, args.instances)\n",
    "    labels_path = os.path.join(outputs_path, args.labels)\n",
    "\n",
    "    # Create the outputs folder.\n",
    "    os.makedirs(outputs_path, exist_ok=True)\n",
    "\n",
    "    # Load the data.\n",
    "    print('Reading {}'.format(data_path))\n",
    "    train = pd.read_csv(data_path, sep='\\t', encoding='latin1')\n",
    "\n",
    "    # Limit the number of training duplicate matches.\n",
    "    train = train[train.n < args.match]\n",
    "\n",
    "    # Define the input data columns.\n",
    "    feature_columns = ['Text_x', 'Text_y']\n",
    "    label_column = 'Label'\n",
    "    group_column = 'Id_x'\n",
    "    answerid_column = 'AnswerId_y'\n",
    "    name_columns = ['Id_x', 'Id_y']\n",
    "    weight_column = 'Weight'\n",
    "\n",
    "    # Report on the dataset.\n",
    "    print('train: {:,} rows with {:.2%} matches'.format(\n",
    "        train.shape[0], train[label_column].mean()))\n",
    "    \n",
    "    # Compute instance weights.\n",
    "    if args.unweighted:\n",
    "        print('No sample weights.')\n",
    "        weight = pd.Series([1.0], train[label_column].unique())\n",
    "    else:\n",
    "        print('Using sample weights.')\n",
    "        label_counts = train[label_column].value_counts()\n",
    "        weight = train.shape[0]/(label_counts.shape[0]*label_counts)\n",
    "        print(weight)\n",
    "    train[weight_column] = train[label_column].apply(lambda x: weight[x])\n",
    "\n",
    "    # Select and format the training data.\n",
    "    train_X = train[feature_columns]\n",
    "    train_y = train[label_column]\n",
    "    sample_weight = train[weight_column]\n",
    "    groups = train[group_column]\n",
    "    names = train[name_columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the featurization and estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append TrainTestClassifier.py\n",
    "\n",
    "    print('Define the model pipeline.')\n",
    "\n",
    "    # Select the training hyperparameters.\n",
    "    n_estimators = args.estimators\n",
    "    min_child_samples = args.min_child_samples\n",
    "    if args.ngrams > 0:\n",
    "        ngram_range = (1, args.ngrams)\n",
    "    else:\n",
    "        ngram_range = None\n",
    "\n",
    "    # Verify that the hyperparameter values are valid.\n",
    "    assert n_estimators > 0\n",
    "    assert min_child_samples > 1\n",
    "    assert ngram_range is not None\n",
    "    assert type(ngram_range) is tuple and len(ngram_range) == 2\n",
    "    assert ngram_range[0] > 0 and ngram_range[0] <= ngram_range[1]\n",
    "\n",
    "    # Define the featurization pipeline.\n",
    "    featurization = [\n",
    "        (column,\n",
    "         make_pipeline(ItemSelector(column),\n",
    "                       text.TfidfVectorizer(ngram_range=ngram_range)))\n",
    "        for column in feature_columns]\n",
    "    features = FeatureUnion(featurization)\n",
    "\n",
    "    # Define the estimator.\n",
    "    estimator = lgb.LGBMClassifier(n_estimators=n_estimators,\n",
    "                                   min_child_samples=min_child_samples,\n",
    "                                   verbose=args.verbose)\n",
    "\n",
    "    # Put them together into the model pipeline.\n",
    "    model = Pipeline([\n",
    "        ('features', features),\n",
    "        ('model', estimator)\n",
    "    ])\n",
    "    \n",
    "    # Report the featurization.\n",
    "    print('Estimators={:,}'.format(n_estimators))\n",
    "    print('Ngram range={}'.format(ngram_range))\n",
    "    print('Min child samples={}'.format(min_child_samples))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append TrainTestClassifier.py\n",
    "\n",
    "    print('Fitting the model.')\n",
    "\n",
    "    # Fit the model.\n",
    "    model.fit(train_X, train_y, model__sample_weight=sample_weight)\n",
    "\n",
    "    # Write the model to file.\n",
    "    if args.save:\n",
    "        joblib.dump(model, model_path)\n",
    "        print('{}: {:.2f} MB'.format(\n",
    "            model_path, os.path.getsize(model_path)/(2**20)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the test data using the model\n",
    "This produces a dataframe of scores with one row per duplicate question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append TrainTestClassifier.py\n",
    "\n",
    "    print('Scoring the test data.')\n",
    "\n",
    "    # Read the test data.\n",
    "    print('Reading {}'.format(test_path))\n",
    "    test = pd.read_csv(test_path, sep='\\t', encoding='latin1')\n",
    "    print('test {:,} rows with {:.2%} matches'.format(\n",
    "        test.shape[0], test[label_column].mean()))\n",
    "\n",
    "    # Collect the model predictions.\n",
    "    test_X = test[feature_columns]\n",
    "    test['probabilities'] = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    # Order the testing data by dupe Id and question AnswerId.\n",
    "    test.sort_values([group_column, answerid_column], inplace=True)\n",
    "\n",
    "    # Extract the ordered probabilities.\n",
    "    probabilities = (\n",
    "        test.probabilities\n",
    "        .groupby(test[group_column], sort=False)\n",
    "        .apply(lambda x: tuple(x.values)))\n",
    "\n",
    "    # Get the individual records.\n",
    "    output_columns_x = ['Id_x', 'AnswerId_x', 'Text_x']\n",
    "    test_score = (test[output_columns_x]\n",
    "                  .drop_duplicates()\n",
    "                  .set_index(group_column))\n",
    "    test_score['probabilities'] = probabilities\n",
    "    test_score.reset_index(inplace=True)\n",
    "    test_score.columns = ['Id', 'AnswerId', 'Text', 'probabilities']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the model's performance statistics on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to TrainTestClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append TrainTestClassifier.py\n",
    "\n",
    "    print(\"Evaluating the model's performance.\")\n",
    "    \n",
    "    # Collect the ordered AnswerId for computing the scores.\n",
    "    labels = sorted(train[answerid_column].unique())\n",
    "    label_order = pd.DataFrame({'label': labels})\n",
    "\n",
    "    # Rank the correct answers.\n",
    "    test_score['Ranks'] = test_score.apply(lambda x:\n",
    "                                           label_rank(x.AnswerId,\n",
    "                                                      x.probabilities,\n",
    "                                                      label_order.label),\n",
    "                                           axis=1)\n",
    "\n",
    "    # Compute the number of correctly ranked answers\n",
    "    for i in range(1, args.rank+1):\n",
    "        print('Accuracy @{} = {:.2%}'.format(\n",
    "            i, (test_score['Ranks'] <= i).mean()))\n",
    "    mean_rank = test_score['Ranks'].mean()\n",
    "    print('Mean Rank {:.4f}'.format(mean_rank))\n",
    "\n",
    "    # Write the scored instances.\n",
    "    if args.save:\n",
    "        test_score.to_csv(instances_path, sep='\\t', index=False,\n",
    "                          encoding='latin1')\n",
    "        label_order.to_csv(labels_path, sep='\\t', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the script to see that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare the training data.\n",
      "Reading ./balanced_pairs_train.tsv\n",
      "train: 33,415 rows with 20.00% matches\n",
      "Using sample weights.\n",
      "0    0.625\n",
      "1    2.500\n",
      "Name: Label, dtype: float64\n",
      "Define the model pipeline.\n",
      "Estimators=1,000\n",
      "Ngram range=(1, 2)\n",
      "Min child samples=10\n",
      "Fitting the model.\n",
      "Scoring the test data.\n",
      "Reading ./balanced_pairs_test.tsv\n",
      "test 287,014 rows with 0.55% matches\n",
      "Evaluating the model's performance.\n",
      "Accuracy @1 = 43.63%\n",
      "Accuracy @2 = 58.91%\n",
      "Accuracy @3 = 65.31%\n",
      "Mean Rank 7.4350\n",
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :     719.98 s.\n",
      "  System :       2.81 s.\n",
      "Wall time:     200.52 s.\n"
     ]
    }
   ],
   "source": [
    "%run -t TrainTestClassifier.py --match 5 --estimators 1000 --ngrams 2 --min_child_samples 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [the next notebook](02_Configure_Batch_AI.ipynb), we create a file to contain the Batch AI configuration we will use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLBatchAIHyperparameterTuning]",
   "language": "python",
   "name": "conda-env-MLBatchAIHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
