{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Local Run\n",
    "In this notebook, we create an Azure ML workspace, and use it to locally run the training script.\n",
    "## Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from azureml.core import Workspace, Experiment, ScriptRunConfig\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import ComputeTarget, BatchAiCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.estimator import Estimator\n",
    "import azureml.core\n",
    "print('azureml.core.VERSION={}'.format(azureml.core.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Azure ML workspace\n",
    "Read in the the workspace created in a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a Batch AI cluster\n",
    "Define the properties of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchai_cluster_name = 'mabouhype'\n",
    "provisioning_config = BatchAiCompute.provisioning_configuration(\n",
    "        vm_size='Standard_D4_v2',\n",
    "        cluster_min_nodes=0,\n",
    "        cluster_max_nodes=16,\n",
    "        autoscale_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a configured Batch AI cluster, if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batchai_cluster_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[batchai_cluster_name]\n",
    "    if type(compute_target) is not BatchAiCompute:\n",
    "        raise Exception('Compute target {} is not a Batch AI cluster.'\n",
    "                        .format(batchai_cluster_name))\n",
    "    print('Using pre-existing Batch AI cluster {}'\n",
    "         .format(batchai_cluster_name))\n",
    "else:\n",
    "    # Create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n",
    "\n",
    "    # You can poll for a minimum number of nodes and set a specific timeout. \n",
    "    # If min node count is provided, priovisioning will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a detailed view of BatchAI cluster status.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.Series(compute_target.get_status().serialize()).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir=os.path.join('.', 'data'), target_path='data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an estimator that specifies the location of the script, sets up its parameters, including the location of the data, defines the compute target, and specifies the packages needed to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator(source_directory=os.path.join('.', 'scripts'), \n",
    "                entry_script='TrainTestClassifier.py',\n",
    "                script_params={'--data-folder': ds.as_mount(),\n",
    "                               '--estimators': '1000',\n",
    "                               '--match': '5',\n",
    "                               '--ngrams': '2',\n",
    "                               '--min_child_samples': '10'},\n",
    "                compute_target=compute_target,\n",
    "                conda_packages=['pandas==0.23.4',\n",
    "                                'scikit-learn==0.20.0'],\n",
    "                pip_packages=[# 'azureml-sdk',\n",
    "                              'lightgbm==2.1.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an experiment to run the script; create it if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='mabouhypelocal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the script to be run. This should return almost immediately, and the value will be a run object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment returns a table with a link to the `Details Page` in the Azure Portal. That page will let you monitor the status of this run of the experiment, and that of previous runs of that experiment. By clicking on a particular run, you can see its details, files output by the script, and the logs of the run, including the `driver.log` with the script's print outs.\n",
    "\n",
    "Get an object associated with the latest run. Using this object, you can programmatically control the job. This object was the value returned by the `exp.submit(src)` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = list(exp.get_runs())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the run to complete. This returns a `dict` with detailed information about the run. Here, we see that the run has `Completed`. Other states include `Running` and `Failed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_status = run.wait_for_completion()\n",
    "run_status['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the metrics logged by the script during its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLBatchAIHyperparameterTuning]",
   "language": "python",
   "name": "conda-env-MLBatchAIHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
