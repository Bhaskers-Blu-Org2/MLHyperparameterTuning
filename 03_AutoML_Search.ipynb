{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "In this notebook, we create an AML cluster, and use it to search for the best set of hyperparameters for the model.\n",
    "\n",
    "The steps in this notebook are\n",
    "- [import libraries](#import),\n",
    "- [read in the Azure ML workspace](#workspace),\n",
    "- [create an AML cluster](#cluster),\n",
    "- [upload the data to the cloud](#upload),\n",
    "- [define a hyperparameter search configuration](#configuration),\n",
    "- [create an estimator](#estimator),\n",
    "- [submit the estimator](#submit), and\n",
    "- [get the results](#results).\n",
    "\n",
    "## Imports  <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml.core.VERSION=1.0.43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration, DataReferenceConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, PrimaryMetricGoal, HyperDriveRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "import azureml.core\n",
    "from get_auth import get_auth\n",
    "print('azureml.core.VERSION={}'.format(azureml.core.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Azure ML workspace <a id='workspace'></a>\n",
    "Read in the the workspace created in a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create Workspace with CLI Authentication\n",
      "Name:\t\thypetuning\n",
      "Location:\teastus2\n"
     ]
    }
   ],
   "source": [
    "auth = get_auth()\n",
    "ws = Workspace.from_config(auth=auth)\n",
    "ws_details = ws.get_details()\n",
    "print('Name:\\t\\t{}\\nLocation:\\t{}'\n",
    "      .format(ws_details['name'],\n",
    "              ws_details['location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an AML cluster <a id='cluster'></a>\n",
    "Define the properties of the cluster needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = 'hypetuning'\n",
    "provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_NC6\", # 'Standard_D4_v2',\n",
    "        # vm_priority = 'lowpriority', # optional\n",
    "        max_nodes=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the configured cluster if it doesn't already exist, or retrieve it if it does exist. Creation can take about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "if cluster_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    if type(compute_target) is not AmlCompute:\n",
    "        raise Exception('Compute target {} is not an AML cluster.'\n",
    "                        .format(cluster_name))\n",
    "    print('Using pre-existing AML cluster {}'.format(cluster_name))\n",
    "else:\n",
    "    # Create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "\n",
    "    # You can poll for a minimum number of nodes and set a specific timeout. \n",
    "    # If min node count is provided, provisioning will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a detailed view of the cluster.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>currentNodeCount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>targetNodeCount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nodeStateCounts</th>\n",
       "      <td>{'preparingNodeCount': 0, 'runningNodeCount': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocationState</th>\n",
       "      <td>Steady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocationStateTransitionTime</th>\n",
       "      <td>2019-06-24T18:15:30.770000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>errors</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creationTime</th>\n",
       "      <td>2019-06-24T18:15:28.557472+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modifiedTime</th>\n",
       "      <td>2019-06-24T18:15:44.250858+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provisioningState</th>\n",
       "      <td>Succeeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provisioningStateTransitionTime</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaleSettings</th>\n",
       "      <td>{'minNodeCount': 0, 'maxNodeCount': 16, 'nodeI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vmPriority</th>\n",
       "      <td>Dedicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vmSize</th>\n",
       "      <td>STANDARD_NC6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Value\n",
       "currentNodeCount                                                                 0\n",
       "targetNodeCount                                                                  0\n",
       "nodeStateCounts                  {'preparingNodeCount': 0, 'runningNodeCount': ...\n",
       "allocationState                                                             Steady\n",
       "allocationStateTransitionTime                     2019-06-24T18:15:30.770000+00:00\n",
       "errors                                                                        None\n",
       "creationTime                                      2019-06-24T18:15:28.557472+00:00\n",
       "modifiedTime                                      2019-06-24T18:15:44.250858+00:00\n",
       "provisioningState                                                        Succeeded\n",
       "provisioningStateTransitionTime                                               None\n",
       "scaleSettings                    {'minNodeCount': 0, 'maxNodeCount': 16, 'nodeI...\n",
       "vmPriority                                                               Dedicated\n",
       "vmSize                                                                STANDARD_NC6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(compute_target.get_status().serialize(), name='Value').to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to the cloud <a id='upload'></a>\n",
    "Prepare the data in X/y form for AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "train_path = os.path.join(data_path, \"balanced_pairs_train.tsv\")\n",
    "tune_path = os.path.join(data_path, \"balanced_pairs_tune.tsv\")\n",
    "train = pd.read_csv(train_path, sep='\\t', encoding='latin1')\n",
    "tune = pd.read_csv(tune_path, sep='\\t', encoding='latin1')\n",
    "feature_columns = [\"Text_x\", \"Text_y\"]\n",
    "label_column = \"Label\"\n",
    "train_X = (train.Text_x + ' ' + train.Text_y)  # train_X = train[feature_columns]\n",
    "train_y = train[label_column]\n",
    "tune_X = (tune.Text_x + ' ' + tune.Text_y)  # tune_X = tune[feature_columns]\n",
    "tune_y = tune[label_column]\n",
    "train_label_counts = train[label_column].value_counts()\n",
    "train_label_weight = train.shape[0] / (train_label_counts.shape[0] * train_label_counts)\n",
    "train_weight = train[label_column].apply(lambda x: train_label_weight[x])\n",
    "tune_label_counts = tune[label_column].value_counts()\n",
    "tune_label_weight = tune.shape[0] / (tune_label_counts.shape[0] * tune_label_counts)\n",
    "tune_weight = tune[label_column].apply(lambda x: tune_label_weight[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the X/y data out to a data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_data_path = \"automl_data\"\n",
    "os.makedirs(automl_data_path, exist_ok=True)\n",
    "\n",
    "train_X_path = os.path.join(automl_data_path, \"train_X.tsv\")\n",
    "train_X.to_csv(train_X_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "train_y_path = os.path.join(automl_data_path, \"train_y.tsv\")\n",
    "train_y.to_csv(train_y_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "train_weight_path = os.path.join(automl_data_path, \"train_weight.tsv\")\n",
    "train_weight.to_csv(train_weight_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "tune_X_path = os.path.join(automl_data_path, \"tune_X.tsv\")\n",
    "tune_X.to_csv(tune_X_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "tune_y_path = os.path.join(automl_data_path, \"tune_y.tsv\")\n",
    "tune_y.to_csv(tune_y_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "tune_weight_path = os.path.join(automl_data_path, \"tune_weight.tsv\")\n",
    "tune_weight.to_csv(tune_weight_path, sep='\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the data in a particular directory on the workspace's default data store. This will show up in the same location in the file system of every job running on the Batch AI cluster.\n",
    "\n",
    "Get a handle to the workspace's default data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data. We use `overwrite=False` to avoid taking the time to re-upload the data should files with the same names be already present. If you change the data and want to refresh what's uploaded, use `overwrite=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./automl_data/train_X.tsv\n",
      "Uploading ./automl_data/train_weight.tsv\n",
      "Uploading ./automl_data/train_y.tsv\n",
      "Uploading ./automl_data/tune_X.tsv\n",
      "Uploading ./automl_data/tune_weight.tsv\n",
      "Uploading ./automl_data/tune_y.tsv\n",
      "Uploaded ./automl_data/tune_y.tsv, 1 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/tune_weight.tsv, 2 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/train_y.tsv, 3 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/train_weight.tsv, 4 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/tune_X.tsv, 5 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/train_X.tsv, 6 files out of an estimated total of 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_0650e3615e794499930e70c712b1d917"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload(src_dir=os.path.join('.', automl_data_path), target_path='data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data reference to download the data to an absolute location on the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore=\"data\", \n",
    "                   path_on_compute=os.path.join(\"/tmp\", \"azureml\"),\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `get_data.py` file in the `scripts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/get_data.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_data():\n",
    "    automl_data_path = os.path.join(\"/tmp\", \"azureml\", \"data\")\n",
    "    \n",
    "    train_X_path = os.path.join(automl_data_path, \"train_X.tsv\")\n",
    "    train_X = pd.read_csv(train_X_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    train_y_path = os.path.join(automl_data_path, \"train_y.tsv\")\n",
    "    train_y = pd.read_csv(train_y_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    train_weight_path = os.path.join(automl_data_path, \"train_weight.tsv\")\n",
    "    train_weight = pd.read_csv(train_weight_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    tune_X_path = os.path.join(automl_data_path, \"tune_X.tsv\")\n",
    "    tune_X = pd.read_csv(tune_X_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    tune_y_path = os.path.join(automl_data_path, \"tune_y.tsv\")\n",
    "    tune_y = pd.read_csv(tune_y_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    tune_weight_path = os.path.join(automl_data_path, \"tune_weight.tsv\")\n",
    "    tune_weight = pd.read_csv(tune_weight_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    data = {\n",
    "        \"X\" : train_X, \"y\" : train_y, \"sample_weight\": train_weight,\n",
    "        \"X_valid\": tune_X, \"y_valid\": tune_y, \"sample_weight_valid\": tune_weight \n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to the Linux DSVM\n",
    "conda_run_config.target = compute_target\n",
    "\n",
    "# set the data reference of the run coonfiguration\n",
    "conda_run_config.data_references = {ds.name: dr}\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd\n",
    "\n",
    "automated_ml_config = AutoMLConfig(task=\"classification\",\n",
    "                                   debug_log=\"dbpedia_auc.log\",\n",
    "                                   path=\"scripts\",\n",
    "                                   data_script=\"get_data.py\",\n",
    "                                   primary_metric=\"AUC_weighted\",\n",
    "                                   run_configuration=conda_run_config,\n",
    "                                   preprocess=True,\n",
    "                                   enable_feature_sweeping=False,\n",
    "                                   iterations=50,\n",
    "                                   iteration_timeout_minutes=90,\n",
    "                                   max_concurrent_iterations=16,\n",
    "                                   max_cores_per_iteration=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the search <a id='submit'></a>\n",
    "Get an experiment to run the search; create it if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='hypetuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the configuration to be run. This should return almost immediately, and the value will be a run object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>hypetuning</td><td>AutoML_420b78eb-bfd3-4cd1-87e3-297f2d02d655</td><td>automl</td><td>Starting</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/hypetuningauto/providers/Microsoft.MachineLearningServices/workspaces/hypetuning/experiments/hypetuning/runs/AutoML_420b78eb-bfd3-4cd1-87e3-297f2d02d655\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: hypetuning,\n",
       "Id: AutoML_420b78eb-bfd3-4cd1-87e3-297f2d02d655,\n",
       "Type: automl,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(automated_ml_config)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment returns a run that when printed shows a table with a link to the `Details Page` in the Azure Portal. That page will let you monitor the status of this run and that of its children runs. By clicking on a particular child run, you can see its details, files output by the script for that configuration, and the logs of the run, including the `driver.log` with the script's print outs.\n",
    "\n",
    "If you want to cancel this trial, run the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the ID of the run in a file. You may use this at a later time to recover the run, as is shown in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = run.id\n",
    "run_id_path = \"run_id.txt\"\n",
    "with open(run_id_path, \"w\") as fp:\n",
    "    fp.write(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until all children runs have either failed or completed, the parent run's status will not be `Completed`. Other possible run statuses include `Preparing`, `Running`, `Finalizing`, and `Failed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preparing'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the RunDetails widget to monitor the execution of the AutoML trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c8b905160b48c5b9ccba580a64148a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_AutoMLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the runs to complete. This returns a `dict` with detailed information about the run. Here, we see that the run has `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "CPU times: user 62.1 ms, sys: 73 µs, total: 62.2 ms\n",
      "Wall time: 5.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_status = run.wait_for_completion()\n",
    "print(run_status['status'])\n",
    "if run_status['status'] != 'Completed':\n",
    "    raise Exception('The run did not successfully complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best model <a id='results'></a>\n",
    "We can automatically select the best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_run = run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>hypetuning</td><td>AutoML_420b78eb-bfd3-4cd1-87e3-297f2d02d655_46</td><td>azureml.scriptrun</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/hypetuningauto/providers/Microsoft.MachineLearningServices/workspaces/hypetuning/experiments/hypetuning/runs/AutoML_420b78eb-bfd3-4cd1-87e3-297f2d02d655_46\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: hypetuning,\n",
       "Id: AutoML_420b78eb-bfd3-4cd1-87e3-297f2d02d655_46,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best model\n",
    "Read in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(data_path, \"balanced_pairs_test.tsv\")\n",
    "test = pd.read_csv(test_path, sep='\\t', encoding='latin1')\n",
    "test_X = (test.Text_x + ' ' + test.Text_y)  # test_X = test[feature_columns]\n",
    "test_y = test[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_data_path = \"automl_data\"\n",
    "\n",
    "test_X_path = os.path.join(automl_data_path, \"test_X.tsv\")\n",
    "test_X.to_csv(test_X_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "test_y_path = os.path.join(automl_data_path, \"test_y.tsv\")\n",
    "test_y.to_csv(test_y_path, sep='\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['probabilities'] = best_run[1].predict_proba(test_X.values)[:, 1]\n",
    "\n",
    "# Order the testing data by dupe Id and question AnswerId.\n",
    "group_column = 'Id_x'\n",
    "answerid_column = 'AnswerId_y'\n",
    "test.sort_values([group_column, answerid_column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group each dupe probabilities for each question.\n",
    "probabilities = (\n",
    "    test.probabilities\n",
    "    .groupby(test[group_column], sort=False)\n",
    "    .apply(lambda x: tuple(x.values)))\n",
    "\n",
    "# Get the individual records.\n",
    "output_columns_x = ['Id_x', 'AnswerId_x', 'Text_x']\n",
    "test_score = (test[output_columns_x]\n",
    "              .drop_duplicates()\n",
    "              .set_index(group_column))\n",
    "test_score['probabilities'] = probabilities\n",
    "test_score.reset_index(inplace=True)\n",
    "test_score.columns = ['Id', 'AnswerId', 'Text', 'probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score_rank(scores):\n",
    "    \"\"\"Compute the ranks of the scores.\"\"\"\n",
    "    return pd.Series(scores).rank(ascending=False)\n",
    "\n",
    "\n",
    "def label_index(label, label_order):\n",
    "    \"\"\"Compute the index of label in label_order.\"\"\"\n",
    "    loc = np.where(label == label_order)[0]\n",
    "    if loc.shape[0] == 0:\n",
    "        return None\n",
    "    return loc[0]\n",
    "\n",
    "\n",
    "def label_rank(label, scores, label_order):\n",
    "    \"\"\"Compute the rank of the true label given the scores of the question labels.\"\"\"\n",
    "    loc = label_index(label, label_order)\n",
    "    if loc is None:\n",
    "        return len(scores) + 1\n",
    "    return score_rank(scores)[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model's performance.\n",
      "Accuracy @1 = 53.96%\n",
      "Accuracy @2 = 69.26%\n",
      "Accuracy @3 = 76.11%\n",
      "Mean Rank 4.3490\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the model's performance.\")\n",
    "\n",
    "# Collect the ordered AnswerId for computing scores.\n",
    "labels = sorted(train[answerid_column].unique())\n",
    "label_order = pd.DataFrame({'label': labels})\n",
    "\n",
    "# Compute the ranks of the correct answers.\n",
    "test_score['Ranks'] = test_score.apply(lambda x:\n",
    "                                       label_rank(x.AnswerId,\n",
    "                                                  x.probabilities,\n",
    "                                                  label_order.label),\n",
    "                                       axis=1)\n",
    "\n",
    "# Compute the number of correctly ranked answers\n",
    "args_rank = 3\n",
    "for i in range(1, args_rank+1):\n",
    "    print('Accuracy @{} = {:.2%}'\n",
    "          .format(i, (test_score['Ranks'] <= i).mean()))\n",
    "mean_rank = test_score['Ranks'].mean()\n",
    "print('Mean Rank {:.4f}'.format(mean_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scoring dataframe that groups each dupe's probabilities and its AnswerIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank = test.groupby(group_column).apply(lambda x: label_rank(x.AnswerId_x.values, x.probabilities.values, x.AnswerId_y.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_rank = 3\n",
    "for i in range(1, args_rank+1):\n",
    "    print('Accuracy @{} = {:.2%}'\n",
    "          .format(i, (test_rank <= i).mean()))\n",
    "mean_rank = test_rank.mean()\n",
    "print('Mean Rank {:.4f}'.format(mean_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupe_ranks(x):\n",
    "    y = pd.Series({'AnswerId': x.AnswerId_x.iloc[0],\n",
    "         'probabilities': tuple(x.probabilities.values),\n",
    "         'AnswerIds': tuple(x.AnswerId_y.values),\n",
    "         'Rank': label_rank(x.AnswerId_x.values, x.probabilities.values, x.AnswerId_y.values)\n",
    "        })\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank = test.groupby(group_column).apply(dupe_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank.Rank.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('datatransformer', DataTransformer(enable_feature_sweeping=None, feature_sweeping_timeout=None,\n",
       "        is_onnx_compatible=None, logger=None, observer=None, task=None)), ('MaxAbsScaler', MaxAbsScaler(copy=True)), ('LightGBMClassifier', LightGBMClassifier(boosting_type='gbdt', class_weight=No...  subsample=0.9405263157894738, subsample_for_bin=200000,\n",
       "          subsample_freq=0, verbose=-10))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline = best_run[1]\n",
    "best_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Pipeline in module sklearn.pipeline object:\n",
      "\n",
      "class Pipeline(sklearn.utils.metaestimators._BaseComposition)\n",
      " |  Pipeline of transforms with a final estimator.\n",
      " |  \n",
      " |  Sequentially apply a list of transforms and a final estimator.\n",
      " |  Intermediate steps of the pipeline must be 'transforms', that is, they\n",
      " |  must implement fit and transform methods.\n",
      " |  The final estimator only needs to implement fit.\n",
      " |  The transformers in the pipeline can be cached using ``memory`` argument.\n",
      " |  \n",
      " |  The purpose of the pipeline is to assemble several steps that can be\n",
      " |  cross-validated together while setting different parameters.\n",
      " |  For this, it enables setting parameters of the various steps using their\n",
      " |  names and the parameter name separated by a '__', as in the example below.\n",
      " |  A step's estimator may be replaced entirely by setting the parameter\n",
      " |  with its name to another estimator, or a transformer removed by setting\n",
      " |  to None.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <pipeline>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  steps : list\n",
      " |      List of (name, transform) tuples (implementing fit/transform) that are\n",
      " |      chained, in the order in which they are chained, with the last object\n",
      " |      an estimator.\n",
      " |  \n",
      " |  memory : None, str or object with the joblib.Memory interface, optional\n",
      " |      Used to cache the fitted transformers of the pipeline. By default,\n",
      " |      no caching is performed. If a string is given, it is the path to\n",
      " |      the caching directory. Enabling caching triggers a clone of\n",
      " |      the transformers before fitting. Therefore, the transformer\n",
      " |      instance given to the pipeline cannot be inspected\n",
      " |      directly. Use the attribute ``named_steps`` or ``steps`` to\n",
      " |      inspect estimators within the pipeline. Caching the\n",
      " |      transformers is advantageous when fitting is time consuming.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  named_steps : bunch object, a dictionary with attribute access\n",
      " |      Read-only attribute to access any step parameter by user given name.\n",
      " |      Keys are step names and values are steps parameters.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.pipeline.make_pipeline : convenience function for simplified\n",
      " |      pipeline construction.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm\n",
      " |  >>> from sklearn.datasets import samples_generator\n",
      " |  >>> from sklearn.feature_selection import SelectKBest\n",
      " |  >>> from sklearn.feature_selection import f_regression\n",
      " |  >>> from sklearn.pipeline import Pipeline\n",
      " |  >>> # generate some data to play with\n",
      " |  >>> X, y = samples_generator.make_classification(\n",
      " |  ...     n_informative=5, n_redundant=0, random_state=42)\n",
      " |  >>> # ANOVA SVM-C\n",
      " |  >>> anova_filter = SelectKBest(f_regression, k=5)\n",
      " |  >>> clf = svm.SVC(kernel='linear')\n",
      " |  >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
      " |  >>> # You can set the parameters using the names issued\n",
      " |  >>> # For instance, fit using a k of 10 in the SelectKBest\n",
      " |  >>> # and a parameter 'C' of the svm\n",
      " |  >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
      " |  ...                      # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      " |  Pipeline(memory=None,\n",
      " |           steps=[('anova', SelectKBest(...)),\n",
      " |                  ('svc', SVC(...))])\n",
      " |  >>> prediction = anova_svm.predict(X)\n",
      " |  >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS\n",
      " |  0.83\n",
      " |  >>> # getting the selected features chosen by anova_filter\n",
      " |  >>> anova_svm.named_steps['anova'].get_support()\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  array([False, False,  True,  True, False, False, True,  True, False,\n",
      " |         True,  False,  True,  True, False, True,  False, True, True,\n",
      " |         False, False])\n",
      " |  >>> # Another way to get selected features chosen by anova_filter\n",
      " |  >>> anova_svm.named_steps.anova.get_support()\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  array([False, False,  True,  True, False, False, True,  True, False,\n",
      " |         True,  False,  True,  True, False, True,  False, True, True,\n",
      " |         False, False])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Pipeline\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, steps, memory=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Apply transforms, and decision_function of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  fit(self, X, y=None, **fit_params)\n",
      " |      Fit the model\n",
      " |      \n",
      " |      Fit all the transforms one after the other and transform the\n",
      " |      data, then fit the transformed data using the final estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of the\n",
      " |          pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : Pipeline\n",
      " |          This estimator\n",
      " |  \n",
      " |  fit_predict(self, X, y=None, **fit_params)\n",
      " |      Applies fit_predict of last step in pipeline after transforms.\n",
      " |      \n",
      " |      Applies fit_transforms of a pipeline to the data, followed by the\n",
      " |      fit_predict method of the final estimator in the pipeline. Valid\n",
      " |      only if the final estimator implements fit_predict.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array-like\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit the model and transform with the final estimator\n",
      " |      \n",
      " |      Fits all the transforms one after the other and transforms the\n",
      " |      data, then uses fit_transform on transformed data with the final\n",
      " |      estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of the\n",
      " |          pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |          Transformed samples\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  predict(self, X, **predict_params)\n",
      " |      Apply transforms to the data, and predict with the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      **predict_params : dict of string -> object\n",
      " |          Parameters to the ``predict`` called at the end of all\n",
      " |          transformations in the pipeline. Note that while this may be\n",
      " |          used to return uncertainties from some models with return_std\n",
      " |          or return_cov, uncertainties that are generated by the\n",
      " |          transformations in the pipeline are not propagated to the\n",
      " |          final estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array-like\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Apply transforms, and predict_log_proba of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Apply transforms, and predict_proba of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_proba : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None)\n",
      " |      Apply transforms, and score with the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Targets used for scoring. Must fulfill label requirements for all\n",
      " |          steps of the pipeline.\n",
      " |      \n",
      " |      sample_weight : array-like, default=None\n",
      " |          If not None, this argument is passed as ``sample_weight`` keyword\n",
      " |          argument to the ``score`` method of the final estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with ``get_params()``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  inverse_transform\n",
      " |      Apply inverse transformations in reverse order\n",
      " |      \n",
      " |      All estimators in the pipeline must support ``inverse_transform``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |          Data samples, where ``n_samples`` is the number of samples and\n",
      " |          ``n_features`` is the number of features. Must fulfill\n",
      " |          input requirements of last step of pipeline's\n",
      " |          ``inverse_transform`` method.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_features]\n",
      " |  \n",
      " |  named_steps\n",
      " |  \n",
      " |  transform\n",
      " |      Apply transforms, and transform with the final estimator\n",
      " |      \n",
      " |      This also works where final estimator is ``None``: all prior\n",
      " |      transformations are applied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to transform. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(best_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('datatransformer',\n",
       "   DataTransformer(enable_feature_sweeping=None, feature_sweeping_timeout=None,\n",
       "           is_onnx_compatible=None, logger=None, observer=None, task=None)),\n",
       "  ('MaxAbsScaler', MaxAbsScaler(copy=True)),\n",
       "  ('LightGBMClassifier',\n",
       "   LightGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "             colsample_bytree=0.8911111111111111, importance_type='split',\n",
       "             learning_rate=0.06842421052631578, max_bin=220, max_depth=-1,\n",
       "             min_child_samples=946, min_child_weight=1,\n",
       "             min_split_gain=0.05263157894736842, n_estimators=800, n_jobs=6,\n",
       "             num_leaves=131, objective=None, random_state=None,\n",
       "             reg_alpha=0.3157894736842105, reg_lambda=1, silent=True,\n",
       "             subsample=0.9405263157894738, subsample_for_bin=200000,\n",
       "             subsample_freq=0, verbose=-10))],\n",
       " 'datatransformer': DataTransformer(enable_feature_sweeping=None, feature_sweeping_timeout=None,\n",
       "         is_onnx_compatible=None, logger=None, observer=None, task=None),\n",
       " 'MaxAbsScaler': MaxAbsScaler(copy=True),\n",
       " 'LightGBMClassifier': LightGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "           colsample_bytree=0.8911111111111111, importance_type='split',\n",
       "           learning_rate=0.06842421052631578, max_bin=220, max_depth=-1,\n",
       "           min_child_samples=946, min_child_weight=1,\n",
       "           min_split_gain=0.05263157894736842, n_estimators=800, n_jobs=6,\n",
       "           num_leaves=131, objective=None, random_state=None,\n",
       "           reg_alpha=0.3157894736842105, reg_lambda=1, silent=True,\n",
       "           subsample=0.9405263157894738, subsample_for_bin=200000,\n",
       "           subsample_freq=0, verbose=-10),\n",
       " 'datatransformer__enable_feature_sweeping': None,\n",
       " 'datatransformer__feature_sweeping_timeout': None,\n",
       " 'datatransformer__is_onnx_compatible': None,\n",
       " 'datatransformer__logger': None,\n",
       " 'datatransformer__observer': None,\n",
       " 'datatransformer__task': None,\n",
       " 'MaxAbsScaler__copy': True,\n",
       " 'LightGBMClassifier__random_state': None,\n",
       " 'LightGBMClassifier__n_jobs': 6,\n",
       " 'LightGBMClassifier__boosting_type': 'gbdt',\n",
       " 'LightGBMClassifier__class_weight': None,\n",
       " 'LightGBMClassifier__colsample_bytree': 0.8911111111111111,\n",
       " 'LightGBMClassifier__importance_type': 'split',\n",
       " 'LightGBMClassifier__learning_rate': 0.06842421052631578,\n",
       " 'LightGBMClassifier__max_depth': -1,\n",
       " 'LightGBMClassifier__min_child_samples': 946,\n",
       " 'LightGBMClassifier__min_child_weight': 1,\n",
       " 'LightGBMClassifier__min_split_gain': 0.05263157894736842,\n",
       " 'LightGBMClassifier__n_estimators': 800,\n",
       " 'LightGBMClassifier__num_leaves': 131,\n",
       " 'LightGBMClassifier__objective': None,\n",
       " 'LightGBMClassifier__reg_alpha': 0.3157894736842105,\n",
       " 'LightGBMClassifier__reg_lambda': 1,\n",
       " 'LightGBMClassifier__silent': True,\n",
       " 'LightGBMClassifier__subsample': 0.9405263157894738,\n",
       " 'LightGBMClassifier__subsample_for_bin': 200000,\n",
       " 'LightGBMClassifier__subsample_freq': 0,\n",
       " 'LightGBMClassifier__max_bin': 220,\n",
       " 'LightGBMClassifier__verbose': -10}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
