{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cluster\n",
    "\n",
    "In this notebook we make use of the Batch AI extensions to generate values for hyperparameters, and create the Batch AI cluster.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import azure.mgmt.batchai.models as models\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.file import FileService\n",
    "sys.path.append('.')\n",
    "import utilities as utils\n",
    "from utilities.job_factory import ParameterSweep, NumericParameter, DiscreteParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell are the names of various files and services used or created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the dotenv file\n",
    "dotenv_path = dotenv.find_dotenv()\n",
    "# The Azure blob container created for the datasets\n",
    "dotenv.set_key(dotenv_path, 'azure_blob_container_name', 'batchaisample')\n",
    "# The Azure blob container directory containing the datasets\n",
    "dotenv.set_key(dotenv_path, 'dataset_path', 'dataset')\n",
    "# The Azure file share created for the scripts and outputs\n",
    "dotenv.set_key(dotenv_path, 'azure_file_share_name', 'batchaisample')\n",
    "# The Azure file share directory containing the Python scripts\n",
    "dotenv.set_key(dotenv_path, 'script_path', 'scripts')\n",
    "# The script to be run\n",
    "dotenv.set_key(dotenv_path, 'script_name', 'TrainTestClassifier.py')\n",
    "# The Batch AI cluster\n",
    "dotenv.set_key(dotenv_path, 'cluster_name', 'd4')\n",
    "# The mount point of the Azure file share in the Docker container\n",
    "dotenv.set_key(dotenv_path, 'azure_file_share_mount_path', 'afs')\n",
    "# The mount point of the Azure blob container in the the Docker container\n",
    "dotenv.set_key(dotenv_path, 'azure_blob_mount_path', 'bfs')\n",
    "# The Batch AI experiment\n",
    "dotenv.set_key(dotenv_path, 'experiment_name', 'hyperparameter_search_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the contents of the `.env` file into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv -o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Python variables used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = os.getenv('configuration_path')\n",
    "azure_blob_container_name = os.getenv('azure_blob_container_name')\n",
    "dataset_path = os.getenv('dataset_path')\n",
    "azure_file_share_name = os.getenv('azure_file_share_name')\n",
    "script_path = os.getenv('script_path')\n",
    "script_name = os.getenv('script_name')\n",
    "cluster_name = os.getenv('cluster_name')\n",
    "azure_file_share_mount_path = os.getenv('azure_file_share_mount_path')\n",
    "azure_blob_mount_path = os.getenv('azure_blob_mount_path')\n",
    "experiment_name = os.getenv('experiment_name')\n",
    "image_name = os.getenv('docker_login') + os.getenv('image_repo') + ':latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Batch AI client\n",
    "Read the configuration, and use it to create a Batch AI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "cfg = utils.config.Configuration(configuration_path)\n",
    "client = utils.config.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the resource group and Batch AI workspace if they do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.config.create_resource_group(cfg)\n",
    "_ = client.workspaces.create(cfg.resource_group, cfg.workspace, cfg.location).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy training datasets and script to Azure storage\n",
    "\n",
    "### Azure blob container\n",
    "\n",
    "We create a blob container named `batchaisample` in your storage account for storing the training and testing datasets.\n",
    "\n",
    "**Note** You don't need to create new blob container for every cluster. We are doing this here to simplify resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service = BlockBlobService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "blob_service.create_container(azure_blob_container_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the TSVs created by the [data prep notebook](00_Data_Prep.ipynb) to an Azure blob container directory named `dataset` using the Azure SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = glob.glob('*.tsv')\n",
    "for file in dataset_files:\n",
    "    print(file)\n",
    "    blob_service.create_blob_from_path(azure_blob_container_name, \n",
    "                                       dataset_path + '/' + file,\n",
    "                                       file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure file share\n",
    "\n",
    "We create a file share named `batchaisample` in your storage account to hold the training script file created in the [create model notebook](01_Create_Model.ipynb). This will also contain the output files created by the running script.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing here to simplify resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "file_service.create_share(azure_file_share_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training script to file share scripts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_service.create_directory(\n",
    "    azure_file_share_name, script_path, fail_on_exist=False)\n",
    "file_service.create_file_from_path(\n",
    "    azure_file_share_name, script_path, script_name, script_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Azure Batch AI compute cluster\n",
    "\n",
    "We will be creating a compute cluster named `d4` with `nodes_count` nodes of type `Standard_D4_v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_count = 4\n",
    "vm_size = 'Standard_D4_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the cluster configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_parameters = models.ClusterCreateParameters(\n",
    "    vm_size=vm_size,\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password or None,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key or None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cfg.workspace, cluster_name, cluster_parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the just created cluster. The `utilities` module contains a helper function to print out a detailed status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)\n",
    "utils.cluster.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter Sweeping using Grid Search\n",
    "We specify the Docker image that will be used to create the containers run in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_settings = models.ContainerSettings(\n",
    "    image_source_registry=models.ImageSourceRegistry(image=image_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the mount points to be created in each container. These will give the container access to the datasets and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            azure_file_url='https://{0}.file.core.windows.net/{1}'.format(\n",
    "                cfg.storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share_mount_path)\n",
    "    ],\n",
    "    azure_blob_file_systems=[\n",
    "        models.AzureBlobFileSystemReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            container_name=azure_blob_container_name,\n",
    "            relative_mount_path=azure_blob_mount_path)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the locations in a container's file system for\n",
    "- storing the job's standard output and error,\n",
    "- obtaining the datasets, and\n",
    "- storing the job's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_out_err_path_prefix = '$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share_mount_path)\n",
    "\n",
    "input_directories = [\n",
    "    models.InputDirectory(\n",
    "        id='SCRIPT',\n",
    "        path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_blob_mount_path, dataset_path))\n",
    "]\n",
    "output_directories = [\n",
    "    models.OutputDirectory(\n",
    "        id='ALL',\n",
    "        path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share_mount_path))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_file_path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}/{2}'.format(azure_file_share_mount_path, \n",
    "                                                                        script_path, \n",
    "                                                                        script_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define specifications for the hyperparameters, and use them to create a parameter substitution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_specs = [\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"NGRAMS\",\n",
    "        values=list(range(1, 6))\n",
    "    ),\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"MATCH\",\n",
    "        values=list(range(10, 50, 10))\n",
    "    ),\n",
    "]\n",
    "\n",
    "parameters = ParameterSweep(param_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the command line arguments that will be passed to the training script. We will use the parameter substitution object to specify where we would like to substitute the values of the parameters in the command line. Note that `parameters` is used like a dict, with the `parameter_name` being used as the key to specify which parameter to substitute. When `parameters.generate_jobs` is called below, the `parameters[name]` variables will be replaced with actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_line_args = '--inputs $AZ_BATCHAI_INPUT_SCRIPT --outputs $AZ_BATCHAI_OUTPUT_ALL'\\\n",
    "    ' --estimators 100 --ngrams {0} --match {1}'.format(\n",
    "    parameters['NGRAMS'], parameters['MATCH'])  # Substitute hyperparameters\n",
    "print(command_line_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the script path and command line arguments together in a module settings structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_toolkit_settings = models.CustomToolkitSettings(\n",
    "        command_line=' '.join(['python', python_script_file_path, command_line_args]),\n",
    "    )\n",
    "print(custom_toolkit_settings.command_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put together the information we just created into a set of job control parameters that will be used by `parameters.generate_jobs_hyperparameter_search` to create the definitions of the jobs to execute on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcp = models.JobCreateParameters(\n",
    "    cluster=models.ResourceId(id=cluster.id),\n",
    "    node_count=1,\n",
    "    std_out_err_path_prefix=std_out_err_path_prefix,\n",
    "    input_directories=input_directories,\n",
    "    output_directories=output_directories,\n",
    "    mount_volumes=mount_volumes,\n",
    "    container_settings=container_settings,\n",
    "    custom_toolkit_settings=custom_toolkit_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a list of jobs to submit using randomly selected combinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_to_submit, param_combinations = parameters.generate_jobs(jcp)\n",
    "for idx, comb in enumerate(param_combinations, 1):\n",
    "    print(\"Parameters {0}: {1}\".format(idx, comb))\n",
    "print(jobs_to_submit[0].custom_toolkit_settings.command_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment called ```hyperparameter_search_experiment```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the jobs to the experiment, and wait for them to complete. This should take about five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_utils = utils.experiment.ExperimentUtils(client, cfg.resource_group, cfg.workspace, experiment_name)\n",
    "jobs = experiment_utils.submit_jobs(jobs_to_submit, 'hyperparam_job2').result()\n",
    "experiment_utils.wait_all_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an extractor that pulls desired metric from each job's log file. \n",
    "- In this example, we extract the number between \"`INFO:root:Accuracy @3 =`\" and \"`%`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_extractor = utils.job.MetricExtractor(\n",
    "                        output_dir_id='ALL',\n",
    "                        logfile='TrainTestClassifier.log',\n",
    "                        regex='INFO:root:Accuracy @3 = (.*?)\\%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the metric values from the log files of the finished jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metrics from the jobs\n",
    "results = experiment_utils.get_metrics_for_jobs(jobs, metric_extractor)\n",
    "results.sort(key=lambda r: r['metric_value'], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(\"Job {0} completed with metric value {1}\".format(result['job_name'], result['metric_value']))\n",
    "    \n",
    "# Print the best job\n",
    "print(\"Best job: {0} with parameters {1}\".format(\n",
    "    results[0]['job_name'], \n",
    "    {ev.name:ev.value for ev in results[0]['job'].environment_variables}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{**{'job_name': result['job_name'], 'metric_value': result['metric_value']},\n",
    "  **{ev.name[6:]:int(ev.value) for ev in result['job'].environment_variables}}\n",
    " for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{**{'a': 10}, **{'a': 20}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Experiment\n",
    "Delete the experiment and jobs inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.experiments.delete(cfg.resource_group, cfg.workspace, experiment_name).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.delete(cfg.resource_group, cfg.workspace, cluster_name).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MLBatchAIHyperparameterTuning]",
   "language": "python",
   "name": "conda-env-MLBatchAIHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
