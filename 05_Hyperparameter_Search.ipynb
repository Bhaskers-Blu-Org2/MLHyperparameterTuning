{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cluster\n",
    "\n",
    "In this notebook we make use of the Batch AI extensions to generate values for hyperparameters, and create the Batch AI cluster.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import dotenv\n",
    "import azure.mgmt.batchai.models as models\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.file import FileService\n",
    "sys.path.append('.')\n",
    "import utilities as utils\n",
    "from utilities.job_factory import ParameterSweep, NumericParameter, DiscreteParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell are the names of various files and services used or created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the dotenv file\n",
    "dotenv_path = dotenv.find_dotenv()\n",
    "# The mount point of the Azure file share in the Docker container\n",
    "dotenv.set_key(dotenv_path, 'azure_file_share_mount_path', 'afs')\n",
    "# The mount point of the Azure blob container in the the Docker container\n",
    "dotenv.set_key(dotenv_path, 'azure_blob_mount_path', 'bfs')\n",
    "# The Batch AI experiment\n",
    "dotenv.set_key(dotenv_path, 'experiment_name', 'hyperparameter_search_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the contents of the `.env` file into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv -o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Python variables used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = os.getenv('configuration_path')\n",
    "image_name = os.getenv('docker_login') + os.getenv('image_repo') + ':latest'\n",
    "azure_blob_container_name = os.getenv('azure_blob_container_name')\n",
    "dataset_path = os.getenv('dataset_path')\n",
    "azure_file_share_name = os.getenv('azure_file_share_name')\n",
    "script_path = os.getenv('script_path')\n",
    "script_name = os.getenv('script_name')\n",
    "azure_file_share_mount_path = os.getenv('azure_file_share_mount_path')\n",
    "azure_blob_mount_path = os.getenv('azure_blob_mount_path')\n",
    "experiment_name = os.getenv('experiment_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Batch AI client\n",
    "Read the configuration, and use it to create a Batch AI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "cfg = utils.config.Configuration(configuration_path)\n",
    "client = utils.config.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the hyperparameter sweeping experiment\n",
    "Specify the Docker image used to create the Docker containers that run the experiment's jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_settings = models.ContainerSettings(\n",
    "    image_source_registry=models.ImageSourceRegistry(image=image_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the mount points to be created in each Docker container. These will give the containers access to the datasets and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            azure_file_url='https://{0}.file.core.windows.net/{1}'.format(\n",
    "                cfg.storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share_mount_path)\n",
    "    ],\n",
    "    azure_blob_file_systems=[\n",
    "        models.AzureBlobFileSystemReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            container_name=azure_blob_container_name,\n",
    "            relative_mount_path=azure_blob_mount_path)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the locations in a container's file system for\n",
    "- storing the job's standard output and error,\n",
    "- obtaining the datasets, and\n",
    "- storing the job's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_out_err_path_prefix = '$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share_mount_path)\n",
    "\n",
    "input_directories = [\n",
    "    models.InputDirectory(\n",
    "        id='SCRIPT',\n",
    "        path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_blob_mount_path, dataset_path))\n",
    "]\n",
    "\n",
    "output_directories = [\n",
    "    models.OutputDirectory(\n",
    "        id='ALL',\n",
    "        path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share_mount_path))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_file_path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}/{2}'.format(azure_file_share_mount_path, \n",
    "                                                                        script_path, \n",
    "                                                                        script_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define specifications for the hyperparameters, and use them to create a parameter substitution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_specs = [\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"NGRAMS\",\n",
    "        values=list(range(1, 6))\n",
    "    ),\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"MATCH\",\n",
    "        values=list(range(10, 50, 10))\n",
    "    ),\n",
    "]\n",
    "\n",
    "parameters = ParameterSweep(param_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the command line arguments that will be passed to the training script. We will use the parameter substitution object to specify where we would like to substitute the values of the parameters in the command line. Note that `parameters` is used like a dict, with the `parameter_name` being used as the key to specify which parameter to substitute. When `parameters.generate_jobs` is called below, the `parameters[name]` variables will be replaced with actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_line_args = '--inputs $AZ_BATCHAI_INPUT_SCRIPT --outputs $AZ_BATCHAI_OUTPUT_ALL'\\\n",
    "    ' --estimators 100 --ngrams {0} --match {1}'.format(\n",
    "    parameters['NGRAMS'], parameters['MATCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the script path and command line arguments together in a module settings structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_toolkit_settings = models.CustomToolkitSettings(\n",
    "        command_line=' '.join(['python', python_script_file_path, command_line_args]),\n",
    "    )\n",
    "print(custom_toolkit_settings.command_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put together the information we just created into a set of job control parameters that will be used by `parameters.generate_jobs` to create the definitions of the jobs to execute on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcp = models.JobCreateParameters(\n",
    "    cluster=models.ResourceId(id=cluster.id),\n",
    "    node_count=1,\n",
    "    std_out_err_path_prefix=std_out_err_path_prefix,\n",
    "    input_directories=input_directories,\n",
    "    output_directories=output_directories,\n",
    "    mount_volumes=mount_volumes,\n",
    "    container_settings=container_settings,\n",
    "    custom_toolkit_settings=custom_toolkit_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the jobs to be run\n",
    "Next, we generate a list of jobs to submit, each with a combinations of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_to_submit, param_combinations = parameters.generate_jobs(jcp)\n",
    "for idx, comb in enumerate(param_combinations, 1):\n",
    "    print(\"Parameters {0}: {1}\".format(idx, comb))\n",
    "print(jobs_to_submit[0].custom_toolkit_settings.command_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment called ```hyperparameter_search_experiment```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the jobs to the experiment, and wait for them to complete. This should take about five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_utils = utils.experiment.ExperimentUtils(client, cfg.resource_group, cfg.workspace, experiment_name)\n",
    "jobs = experiment_utils.submit_jobs(jobs_to_submit, 'hyperparam_job2').result()\n",
    "experiment_utils.wait_all_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an extractor that pulls desired metric from each job's log file. \n",
    "- In this example, we extract the number between \"`INFO:root:Accuracy @3 =`\" and \"`%`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_extractor = utils.job.MetricExtractor(\n",
    "                        output_dir_id='ALL',\n",
    "                        logfile='TrainTestClassifier.log',\n",
    "                        regex='INFO:root:Accuracy @3 = (.*?)\\%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the metric values from the log files of the finished jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each job's metrics\n",
    "results = experiment_utils.get_metrics_for_jobs(jobs, metric_extractor)\n",
    "\n",
    "# Sort them in decreasing order.\n",
    "results.sort(key=lambda r: r['metric_value'], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(\"Job {0} completed with metric value {1}\".format(result['job_name'], result['metric_value']))\n",
    "    \n",
    "# Print the best job\n",
    "print(\"Best job: {0} with parameters {1}\".format(\n",
    "    results[0]['job_name'], \n",
    "    {ev.name:ev.value for ev in results[0]['job'].environment_variables}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{**{'job_name': result['job_name'], 'metric_value': result['metric_value']},\n",
    "  **{ev.name[6:]:int(ev.value) for ev in result['job'].environment_variables}}\n",
    " for result in results]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MLBatchAIHyperparameterTuning]",
   "language": "python",
   "name": "conda-env-MLBatchAIHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
