{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script\n",
    "In this notebook, we create the training script whose hyperparameters will be tuned. This script is stored alone in a `scripts` directory both for ease of reference and because the Azure ML SDK limits the contents of this directory to at most 300 MB.\n",
    "\n",
    "The notebook cells are each appended in turn in the training script, so it is essential that you run the notebook's cells _in order_ for the script to run correctly. If you edit this notebook's cells, be sure to preserve the blank lines at the start and end of the cells, as they prevent the contents of consecutive cells from being improperly concatenated.\n",
    "\n",
    "The script sections are\n",
    "- [import libraries](#import),\n",
    "- [define utility functions and classes](#utility),\n",
    "- [define the script input parameters](#parameters),\n",
    "- [load and prepare the training and tuning data](#data),\n",
    "- [define the training pipeline](#pipeline),\n",
    "- [train the model](#train),\n",
    "- [score the test data](#score), and\n",
    "- [compute the test data performance](#performance).\n",
    "\n",
    "[The final cell](#run) runs the script using the training data created by [the first notebook](00_Data_Prep.ipynb).\n",
    "\n",
    "Start by creating the `scripts` directory, if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/TrainTestClassifier.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core import Run\n",
    "import azureml.core\n",
    "print('azureml.core.VERSION={}'.format(azureml.core.VERSION))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions and classes <a id='utility'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile --append scripts/TrainTestClassifier.py\n",
    "\n",
    "def score_rank(scores):\n",
    "    \"\"\"Compute the ranks of the scores.\"\"\"\n",
    "    return pd.Series(scores).rank(ascending=False)\n",
    "\n",
    "\n",
    "def label_index(label, label_order):\n",
    "    \"\"\"Compute the index of label in label_order.\"\"\"\n",
    "    loc = np.where(label == label_order)[0]\n",
    "    if loc.shape[0] == 0:\n",
    "        return None\n",
    "    return loc[0]\n",
    "\n",
    "\n",
    "def label_rank(label, scores, label_order):\n",
    "    \"\"\"Compute the rank of label using the scores.\"\"\"\n",
    "    loc = label_index(label, label_order)\n",
    "    if loc is None:\n",
    "        return len(scores) + 1\n",
    "    return score_rank(scores)[loc]\n",
    "\n",
    "\n",
    "def log_evaluation(logger, metric_name, period=1):\n",
    "    \"\"\"Create a callback that logs the evaluation results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logger : function\n",
    "    period : int, optional (default=1)\n",
    "        The period to print the evaluation results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    callback : function\n",
    "        The callback that logs the evaluation results every ``period`` iteration(s).\n",
    "    \"\"\"\n",
    "    def callback(env):\n",
    "        \"\"\"internal function\"\"\"\n",
    "        if period > 0 and env.evaluation_result_list and (env.iteration + 1) % period == 0:\n",
    "            logger(metric_name,\n",
    "                   1-env.evaluation_result_list[0][2])\n",
    "    callback.order = 10\n",
    "    return callback\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='lightgbm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input parameters <a id='parameters'></a>\n",
    "One of the most important parameters is `estimators`, the number of estimators that allows you to trade-off accuracy, modeling time, and model size. The table below should give you an idea of the relationships between the number of estimators and the metrics. The default value is 100.\n",
    "\n",
    "| Estimators | Run time (s) | Size (MB) | Accuracy@1 | Accuracy@2 | Accuracy@3 |\n",
    "|------------|--------------|-----------|------------|------------|------------|\n",
    "|        100 |           40 |  2 | 25.02% | 38.72% | 47.83% |\n",
    "|       1000 |          177 |  4 | 46.79% | 60.80% | 69.11% |\n",
    "|       2000 |          359 |  7 | 51.38% | 65.93% | 73.09% |\n",
    "|       4000 |          628 | 12 | 53.39% | 67.40% | 74.74% |\n",
    "|       8000 |          904 | 22 | 54,62% | 67.77% | 75.35% |\n",
    "\n",
    "Other parameters that may be useful to tune include the following:\n",
    "* `ngrams`: the maximum n-gram size for features, an integer ranging from 1 (default 1),\n",
    "* `min_child_samples`: the minimum number of samples in a leaf, an integer ranging from 1 (default 20),\n",
    "* `match`: the maximum number of training examples per duplicate question, an integer ranging from 2 (default 10), and\n",
    "* `unweighted`: whether to use sample weights to compensate for unbalanced data, a boolean (default weighted).\n",
    "\n",
    "The performance of the estimator is estimated on held-aside test data, and the statistic reported is how far down the list of sorted results is the correct result found. The `rank` parameter controls the maximum distance down the list for which the statistic is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile --append scripts/TrainTestClassifier.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Fit and evaluate a model'\n",
    "                                     ' based on train-test datasets.')\n",
    "    parser.add_argument('--data-folder', help='the path to the data',\n",
    "                        dest='data_folder', default='.')\n",
    "    parser.add_argument('--inputs', help='the inputs directory',\n",
    "                        default='data')\n",
    "    parser.add_argument('--data', help='the training dataset name',\n",
    "                        default='balanced_pairs_train.tsv')\n",
    "    parser.add_argument('--tune', help='the tune dataset name',\n",
    "                        default='balanced_pairs_tune.tsv')\n",
    "    parser.add_argument('--test', help='the test dataset name',\n",
    "                        default='balanced_pairs_test.tsv')\n",
    "    parser.add_argument('--estimators',\n",
    "                        help='the number of learner estimators',\n",
    "                        type=int, default=100)\n",
    "    parser.add_argument('--min_child_samples',\n",
    "                        help='the minimum number of samples in a child(leaf)',\n",
    "                        type=int, default=20)\n",
    "    parser.add_argument('--ngrams',\n",
    "                        help='the maximum size of word ngrams',\n",
    "                        type=int, default=1)\n",
    "    parser.add_argument('--match',\n",
    "                        help='the maximum number of duplicate matches',\n",
    "                        type=int, default=20)\n",
    "    parser.add_argument('--unweighted',\n",
    "                        help='whether or not to use instance weights',\n",
    "                        default='No')\n",
    "    parser.add_argument('--rank',\n",
    "                        help='the maximum rank of correct answers',\n",
    "                        type=int, default=3)\n",
    "    parser.add_argument('--outputs', help='the outputs directory',\n",
    "                        default='outputs')\n",
    "    parser.add_argument('--save', help='the model file base name', default='None')\n",
    "    parser.add_argument('--verbose',\n",
    "                        help='the verbosity of the estimator',\n",
    "                        type=int, default=-1)\n",
    "    args = parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the training data <a id='data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile --append scripts/TrainTestClassifier.py\n",
    "\n",
    "    # Get a run logger.\n",
    "    run = Run.get_context()\n",
    "\n",
    "    # What to name the metric logged\n",
    "    metric_name = \"accuracy\"\n",
    "\n",
    "    print('Prepare the training data.')\n",
    "    \n",
    "    # Paths to the input data.\n",
    "    data_path = args.data_folder\n",
    "    inputs_path = os.path.join(data_path, args.inputs)\n",
    "    data_path = os.path.join(inputs_path, args.data)\n",
    "    tune_path = os.path.join(inputs_path, args.tune)\n",
    "    test_path = os.path.join(inputs_path, args.test)\n",
    "\n",
    "    # Paths for the output data.\n",
    "    outputs_path = args.outputs\n",
    "    model_path = os.path.join(outputs_path, '{}.pkl'.format(args.save))\n",
    "    labels_path = os.path.join(outputs_path, '{}.csv'.format(args.save))\n",
    "\n",
    "    # Create the outputs folder.\n",
    "    os.makedirs(outputs_path, exist_ok=True)\n",
    "\n",
    "    # Define the input data columns.\n",
    "    feature_columns = ['Text_x', 'Text_y']\n",
    "    label_column = 'Label'\n",
    "    group_column = 'Id_x'\n",
    "    answerid_column = 'AnswerId_y'\n",
    "    name_columns = ['Id_x', 'Id_y']\n",
    "    weight_column = 'Weight'\n",
    "\n",
    "    # Load the training data.\n",
    "    print('Reading {}'.format(data_path))\n",
    "    train = pd.read_csv(data_path, sep='\\t', encoding='latin1')\n",
    "\n",
    "    # Limit the number of training duplicate matches.\n",
    "    train = train[train.n < args.match]\n",
    "\n",
    "    # Report on the dataset.\n",
    "    print('train: {:,} rows with {:.2%} matches'\n",
    "          .format(train.shape[0], train[label_column].mean()))\n",
    "    \n",
    "    # Load the tunning data.\n",
    "    print('Reading {}'.format(tune_path))\n",
    "    tune = pd.read_csv(tune_path, sep='\\t', encoding='latin1')\n",
    "\n",
    "    # Report on the dataset.\n",
    "    print('tune: {:,} rows with {:.2%} matches'\n",
    "          .format(tune.shape[0], tune[label_column].mean()))\n",
    "    \n",
    "    # Compute instance weights.\n",
    "    if args.unweighted == 'Yes':\n",
    "        print('No sample weights.')\n",
    "        labels = train[label_column].unique()\n",
    "        weight = pd.Series([1.0] * labels.shape[0], labels)\n",
    "    else:\n",
    "        print('Using sample weights.')\n",
    "        label_counts = train[label_column].value_counts()\n",
    "        weight = train.shape[0] / (label_counts.shape[0] * label_counts)\n",
    "        print(weight)\n",
    "    train[weight_column] = train[label_column].apply(lambda x: weight[x])\n",
    "\n",
    "    # Select and format the training data.\n",
    "    train_X = train[feature_columns]\n",
    "    train_y = train[label_column]\n",
    "    train_group = train[group_column]\n",
    "    train_sample_weight = train[weight_column]\n",
    "    train_names = train[name_columns]\n",
    "    tune_X = tune[feature_columns]\n",
    "    tune_y = tune[label_column]\n",
    "    tune_group = tune[group_column]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the featurization and estimator <a id='pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile --append scripts/TrainTestClassifier.py\n",
    "\n",
    "    print('Define the model pipeline.')\n",
    "\n",
    "    # Select the training hyperparameters.\n",
    "    n_estimators = args.estimators\n",
    "    min_child_samples = args.min_child_samples\n",
    "    if args.ngrams > 0:\n",
    "        ngram_range = (1, args.ngrams)\n",
    "    else:\n",
    "        ngram_range = None\n",
    "    accuracy_rank = args.rank\n",
    "\n",
    "    # Verify that the hyperparameter settings are valid.\n",
    "    if n_estimators <= 0:\n",
    "        raise Exception('n_estimators must be > 0')\n",
    "    if min_child_samples <= 0:\n",
    "        raise Exception('min_child_samples must be > 0')\n",
    "    if (ngram_range is None\n",
    "        or type(ngram_range) is not tuple\n",
    "        or len(ngram_range) != 2\n",
    "        or ngram_range[0] < 1\n",
    "        or ngram_range[0] > ngram_range[1]):\n",
    "        raise Exception('ngram_range must be a tuple with two integers (a, b) where a > 0 and a <= b')\n",
    "    if accuracy_rank < 1:\n",
    "        raise Exception(\"accuracy_rank must be at least 1.\")\n",
    "\n",
    "    # Define the featurization pipeline\n",
    "    featurization = [\n",
    "        (column, text.TfidfVectorizer(ngram_range=ngram_range), column)\n",
    "        for column in feature_columns]\n",
    "    features = ColumnTransformer(featurization)\n",
    "\n",
    "    # Define the estimator.\n",
    "    estimator = lgb.LGBMClassifier(n_estimators=n_estimators,\n",
    "                                   min_child_samples=min_child_samples,\n",
    "                                   verbose=args.verbose)\n",
    "\n",
    "    # Put them together into the model pipeline.\n",
    "    model = Pipeline([\n",
    "        ('features', features),\n",
    "        ('estimator', estimator)\n",
    "    ])\n",
    "    \n",
    "    # Report the featurization.\n",
    "    print('Estimators={:,}'.format(n_estimators))\n",
    "    print('Ngram range={}'.format(ngram_range))\n",
    "    print('Min child samples={}'.format(min_child_samples))\n",
    "    print('Accuracy rank={}'.format(accuracy_rank))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model <a id='train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile --append scripts/TrainTestClassifier.py\n",
    "\n",
    "    print('Fitting the model.')\n",
    "\n",
    "    # Collect the ordered AnswerId for computing scores.\n",
    "    labels = sorted(train[answerid_column].unique())\n",
    "    label_order = pd.DataFrame({'label': labels})\n",
    "\n",
    "    # Featurize the train and tune data.  It's important to only fit the\n",
    "    # featurizer on the training data, so that the tuning data is treated the\n",
    "    # same way the testing data will be later on.\n",
    "    train_X_features = model.named_steps[\"features\"].fit_transform(train_X)\n",
    "    tune_X_features = model.named_steps[\"features\"].transform(tune_X)\n",
    "\n",
    "    # Fit the model.\n",
    "    model.named_steps[\"estimator\"].fit(\n",
    "        train_X_features, train_y, sample_weight=train_sample_weight,\n",
    "        feature_name=model.named_steps[\"features\"].get_feature_names(),\n",
    "        eval_set=[(tune_X_features, tune_y)], eval_names=[\"tune\"],\n",
    "        callbacks=[log_evaluation(run.log, metric_name, period=100)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Write the model to file.\n",
    "    if args.save != 'None':\n",
    "        print('Saving the model to {}'.format(model_path))\n",
    "        joblib.dump(model, model_path)\n",
    "        print('{}: {:.2f} MB'\n",
    "              .format(model_path, os.path.getsize(model_path)/(2**20)))\n",
    "        print('Saving the labels to {}'.format(labels_path))\n",
    "        label_order.to_csv(labels_path, sep='\\t', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the test data using the model <a id='score'></a>\n",
    "This produces a dataframe of scores with one row per duplicate question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile --append scripts/TrainTestClassifier.py\n",
    "\n",
    "    print('Scoring the test data.')\n",
    "\n",
    "    # Read the test data.\n",
    "    print('Reading {}'.format(test_path))\n",
    "    test = pd.read_csv(test_path, sep='\\t', encoding='latin1')\n",
    "    print('test: {:,} rows with {:.2%} matches'\n",
    "          .format(test.shape[0], test[label_column].mean()))\n",
    "\n",
    "    # Collect the model predictions.\n",
    "    test_X = test[feature_columns]\n",
    "    test['probabilities'] = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    # Order the testing data by dupe Id and question AnswerId.\n",
    "    test.sort_values([group_column, answerid_column], inplace=True)\n",
    "\n",
    "    # Extract the ordered probabilities.\n",
    "    probabilities = (\n",
    "        test.probabilities\n",
    "        .groupby(test[group_column], sort=False)\n",
    "        .apply(lambda x: tuple(x.values)))\n",
    "\n",
    "    # Get the individual records.\n",
    "    output_columns_x = ['Id_x', 'AnswerId_x', 'Text_x']\n",
    "    test_score = (test[output_columns_x]\n",
    "                  .drop_duplicates()\n",
    "                  .set_index(group_column))\n",
    "    test_score['probabilities'] = probabilities\n",
    "    test_score.reset_index(inplace=True)\n",
    "    test_score.columns = ['Id', 'AnswerId', 'Text', 'probabilities']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the model's performance statistics on the test data <a id='performance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile --append scripts/TrainTestClassifier.py\n",
    "\n",
    "    print(\"Evaluating the model's performance.\")\n",
    "    \n",
    "    # Compute the ranks of the correct answers.\n",
    "    test_score['Ranks'] = test_score.apply(lambda x:\n",
    "                                           label_rank(x.AnswerId,\n",
    "                                                      x.probabilities,\n",
    "                                                      label_order.label),\n",
    "                                           axis=1)\n",
    "\n",
    "    # Compute the number of correctly ranked answers\n",
    "    for i in range(1, args.rank+1):\n",
    "        print('Accuracy @{} = {:.2%}'\n",
    "              .format(i, (test_score['Ranks'] <= i).mean()))\n",
    "    mean_rank = test_score['Ranks'].mean()\n",
    "    print('Mean Rank {:.4f}'.format(mean_rank))\n",
    "\n",
    "    # Log the metric.\n",
    "    accuracy_at_rank = (test_score['Ranks'] <= args.rank).mean()\n",
    "    run.log(metric_name, accuracy_at_rank)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the script to see that it works <a id='run'></a>\n",
    "This should take around five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -t scripts/TrainTestClassifier.py --estimators 1000 --match 5 --ngrams 2 --min_child_samples 10 --save FAQ-ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [the next notebook](02_Run_Locally.ipynb), we set up and use the AML SDK to run the training script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
