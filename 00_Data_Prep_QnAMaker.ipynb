{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for QnA Maker\n",
    "In this notebook, we use a subset of [Stack Exchange network](https://archive.org/details/stackexchange) question data which includes original questions tagged as 'JavaScript', their duplicate questions and their answers. Here, we provide the steps to prepare the data to use for training, tuning, and testing a [QnA Maker](https://www.qnamaker.ai) model that will match a new question with an existing original question. The data files produced are stored in a `data` directory for ease of reference and also to keep them separate from the training script.\n",
    "\n",
    "The data preparation steps are\n",
    "- [import libraries and define parameters](#import),\n",
    "- [ingest the data](#ingest),\n",
    "- [cleanse the data](#cleanse),\n",
    "- [prepare the train, tune, and test datasets](#prepare), and\n",
    "- [save the datasets.](#save)\n",
    "\n",
    "## Imports and parameters <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion <a id='ingest'></a>\n",
    "Next, we the original questions and their answers, and the train, tune, and test duplicate questions sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "questions_path = os.path.join(data_path, \"questions.tsv\")\n",
    "answers_path = os.path.join(data_path, \"answers.tsv\")\n",
    "dupes_train_path = os.path.join(data_path, \"dupes_train.tsv\")\n",
    "dupes_tune_path = os.path.join(data_path, \"dupes_tune.tsv\")\n",
    "dupes_test_path = os.path.join(data_path, \"dupes_test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(questions_path, sep='\\t', encoding='latin1')\n",
    "answers = pd.read_csv(answers_path, sep='\\t', encoding='latin1')\n",
    "dupes_train = pd.read_csv(dupes_train_path, sep='\\t', encoding='latin1')\n",
    "dupes_tune = pd.read_csv(dupes_tune_path, sep='\\t', encoding='latin1')\n",
    "dupes_test = pd.read_csv(dupes_test_path, sep='\\t', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data files for QnA Maker\n",
    "Name the columns of the data to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_columns = [\"Id\", \"AnswerId\", \"Question\", \"Answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the original questions and the training and tuning duplicate questions with their answers. Add the question and answer IDs as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA = (questions[[\"Id\", \"AnswerId\", \"Text0\"]]\n",
    "       .merge(answers[[\"Id\", \"Text0\"]], left_on=\"AnswerId\", right_on=\"Id\")\n",
    "       .drop([\"Id_y\"], axis=1))\n",
    "TnA = (dupes_train[[\"Id\", \"AnswerId\", \"Text0\"]]\n",
    "       .merge(answers[[\"Id\", \"Text0\"]], left_on=\"AnswerId\", right_on=\"Id\")\n",
    "       .drop([\"Id_y\"], axis=1))\n",
    "UnA = (dupes_tune[[\"Id\", \"AnswerId\", \"Text0\"]].reset_index(drop=True)\n",
    "       .merge(answers[[\"Id\", \"Text0\"]], left_on=\"AnswerId\", right_on=\"Id\")\n",
    "       .drop([\"Id_y\"], axis=1))\n",
    "QnA_train = pd.concat([QnA, TnA, UnA])\n",
    "\n",
    "QnA_train.columns = QnA_columns\n",
    "\n",
    "QnA_train[\"Metadata\"] = QnA_train.apply(lambda x: \"|\".join([\"Id:\"+str(x.Id), \"AnswerId:\"+str(x.AnswerId)]), axis=1)\n",
    "\n",
    "QnA_train = QnA_train[[\"Question\", \"Answer\", \"Metadata\", \"Id\", \"AnswerId\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the test duplicate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_test = (dupes_test[[\"Id\", \"AnswerId\", \"Text0\"]].reset_index(drop=True)\n",
    "            .merge(answers[[\"Id\", \"Text0\"]], left_on=\"AnswerId\", right_on=\"Id\")\n",
    "            .drop([\"Id_y\"], axis=1))\n",
    "\n",
    "QnA_test.columns = QnA_columns\n",
    "\n",
    "QnA_test[\"Metadata\"] = QnA_test.apply(lambda x: \"|\".join([\"Id:\"+str(x.Id), \"AnswerId:\"+str(x.AnswerId)]), axis=1)\n",
    "\n",
    "QnA_test = QnA_test[[\"Question\", \"Answer\", \"Metadata\", \"Id\", \"AnswerId\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a version of the training data with only the AnswerId in the Answer column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_noanswer = QnA_train.copy()\n",
    "QnA_noanswer[\"Answer\"] = \"AnswerId is \" + QnA_noanswer.AnswerId.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the training and tuning duplicate questions with their original questions. Add the question and answer IDs as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TnQ = (dupes_train[[\"Id\", \"AnswerId\", \"Text0\"]]\n",
    "       .merge(questions[[\"AnswerId\", \"Text0\"]], on=\"AnswerId\"))\n",
    "UnQ = (dupes_tune[[\"Id\", \"AnswerId\", \"Text0\"]].reset_index(drop=True)\n",
    "       .merge(questions[[\"AnswerId\", \"Text0\"]], on=\"AnswerId\"))\n",
    "QnQ_train = pd.concat([TnQ, UnQ])\n",
    "\n",
    "QnQ_train.columns = QnA_columns\n",
    "\n",
    "QnQ_train[\"Metadata\"] = QnQ_train.apply(lambda x: \"|\".join([\"Id:\"+str(x.Id), \"AnswerId:\"+str(x.AnswerId)]), axis=1)\n",
    "\n",
    "QnQ_train = QnQ_train[[\"Question\", \"Answer\", \"Metadata\", \"Id\", \"AnswerId\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the test duplicate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnQ_test = (dupes_test[[\"Id\", \"AnswerId\", \"Text0\"]].reset_index(drop=True)\n",
    "            .merge(questions[[\"AnswerId\", \"Text0\"]], on=\"AnswerId\"))\n",
    "\n",
    "QnQ_test.columns = QnA_columns\n",
    "\n",
    "QnQ_test[\"Metadata\"] = QnQ_test.apply(lambda x: \"|\".join([\"Id:\"+str(x.Id), \"AnswerId:\"+str(x.AnswerId)]), axis=1)\n",
    "\n",
    "QnQ_test = QnQ_test[[\"Question\", \"Answer\", \"Metadata\", \"Id\", \"AnswerId\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort all dataframes by AnswerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_train.sort_values(\"AnswerId\", inplace=True)\n",
    "QnA_test.sort_values(\"AnswerId\", inplace=True)\n",
    "QnA_noanswer.sort_values(\"AnswerId\", inplace=True)\n",
    "QnQ_train.sort_values(\"AnswerId\", inplace=True)\n",
    "QnQ_test.sort_values(\"AnswerId\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the files to the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_train_path = os.path.join(data_path, 'QnA_train.tsv')\n",
    "print('Writing {:,} rows to {}'.format(QnA_train.shape[0], QnA_train_path))\n",
    "QnA_train.to_csv(QnA_train_path, sep='\\t',header=True, index=False)\n",
    "\n",
    "QnA_test_path = os.path.join(data_path, 'QnA_test.tsv')\n",
    "print('Writing {:,} rows to {}'.format(QnA_test.shape[0], QnA_test_path))\n",
    "QnA_test.to_csv(QnA_test_path, sep='\\t',header=True, index=False)\n",
    "\n",
    "QnA_noanswer_path = os.path.join(data_path, 'QnA_noanswer.tsv')\n",
    "print('Writing {:,} rows to {}'.format(QnA_noanswer.shape[0], QnA_noanswer_path))\n",
    "QnA_noanswer.to_csv(QnA_noanswer_path, sep='\\t',header=True, index=False)\n",
    "\n",
    "QnQ_train_path = os.path.join(data_path, 'QnQ_train.tsv')\n",
    "print('Writing {:,} rows to {}'.format(QnQ_train.shape[0], QnQ_train_path))\n",
    "QnQ_train.to_csv(QnQ_train_path, sep='\\t',header=True, index=False)\n",
    "\n",
    "QnQ_test_path = os.path.join(data_path, 'QnQ_test.tsv')\n",
    "print('Writing {:,} rows to {}'.format(QnQ_test.shape[0], QnQ_test_path))\n",
    "QnQ_test.to_csv(QnQ_test_path, sep='\\t',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
